
# Histology Image weakly supervised Learning Segmentation

This deep learning model is designed to perform weakly supervised learning segmentation on Histology images from The Cancer Genome Atlas (TCGA) slide images. The code uses a combination of gradient-weighted class activation mapping (GradCAM) and dense conditional random fields (denseCRF) to improve the segmentation accuracy of the model.


## Dataset

Downloading TCGA slide image by GDC-client
## Data Preproccessing

**Data loading**: The dataset is loaded using the OpenSlide library, which allows the program to access and read the .svs format of slide images.

**Image resizing**: The loaded slide images are resized to a fixed size of 224x224 pixels, which is the input size of the VGG16 network used as the feature extractor.

**Image normalization**: The pixel values of the resized images are normalized by dividing each pixel by 255. This rescales the pixel values to the range of [0, 1].

**Data augmentation**: The dataset is augmented by randomly flipping the images horizontally and vertically. This helps to increase the size of the dataset and prevent overfitting.

**Patch extraction**: The slide images are divided into smaller patches of size 224x224 pixels, which are then used as inputs to the model. This is done to reduce the memory requirements of the model and allow it to process large slide images.

**Label preparation**: The labels for the slide images are generated using the annotations provided in the TCGA dataset. The annotations are in the form of XML files that contain information about the location and type of each annotation. The program extracts the annotation masks from the XML files and creates binary segmentation masks for each class (tumor and non-tumor). These masks are then resized and divided into smaller patches, which correspond to the patches extracted from the slide images.
## Model

The model used in the code is a modified version of the VGG16 architecture, which is a widely used convolutional neural network for image classification tasks. The VGG16 architecture consists of 13 convolutional layers and 3 fully connected layers. In this code, the last fully connected layer is replaced with a convolutional layer to output a feature map with the same dimensions as the input image.

The feature extractor used in the code is the Grad-CAM (Gradient-weighted Class Activation Mapping) algorithm. Grad-CAM is a technique that generates class activation maps from a pre-trained neural network, indicating which regions of the input image contribute the most to the classification decision of the network. The Grad-CAM algorithm computes the gradients of the final convolutional layer with respect to the target class, and uses these gradients to weigh the feature maps of that layer. The weighted sum of the feature maps results in a class activation map that highlights the regions of the input image that are important for the classification decision.

The class activation map generated by Grad-CAM is then post-processed using a Conditional Random Field (CRF) to refine the segmentation mask. The CRF is a graphical model that models the pairwise relationships between neighboring pixels, and can be used to smooth the segmentation mask and improve its accuracy. The pydensecrf library is used in the code to apply the CRF to the class activation map.
